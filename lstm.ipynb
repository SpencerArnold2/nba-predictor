{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spencer/Desktop/Repos/nba-predictor/preprocessing.py:43: FutureWarning: Dropping of nuisance columns in rolling operations is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the operation. Dropped columns were Index(['matchup', 'team_abbreviation', 'team_name', 'wl'], dtype='object')\n",
      "  rolling_averages = grouped[columns_to_average].apply(lambda x: x.rolling(window=len(x), min_periods=1).mean().shift(1))\n",
      "/Users/spencer/Desktop/Repos/nba-predictor/preprocessing.py:43: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  rolling_averages = grouped[columns_to_average].apply(lambda x: x.rolling(window=len(x), min_periods=1).mean().shift(1))\n",
      "/Users/spencer/Desktop/Repos/nba-predictor/preprocessing.py:62: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('_y$', '')\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "data_path = \"data/game.csv\"\n",
    "X_train, X_test, y_train, y_test = prep_all(data_path)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1, 135)  # Reshape to (32520, 1, 136)\n",
    "X_test = X_test.values.reshape(-1, 1, 135)    # Reshape to (N, 1, 136), where N is the number of test samples\n",
    "\n",
    "train_data = TensorDataset(torch.tensor(X_train), torch.tensor(y_train.to_numpy()))\n",
    "test_data = TensorDataset(torch.tensor(X_test), torch.tensor(y_test.to_numpy()))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[2]  # The number of features in your preprocessed data\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "output_size = 2  # Win or loss (binary classification)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.0363, Validation Acc: 60.54%\n",
      "Epoch [2/20], Loss: 0.0772, Validation Acc: 60.39%\n",
      "Epoch [3/20], Loss: 0.0650, Validation Acc: 60.38%\n",
      "Epoch [4/20], Loss: 0.0558, Validation Acc: 60.48%\n",
      "Epoch [5/20], Loss: 0.0478, Validation Acc: 60.48%\n",
      "Epoch [6/20], Loss: 0.0846, Validation Acc: 60.08%\n",
      "Epoch [7/20], Loss: 0.0166, Validation Acc: 60.26%\n",
      "Epoch [8/20], Loss: 0.0198, Validation Acc: 60.42%\n",
      "Epoch [9/20], Loss: 0.0534, Validation Acc: 60.18%\n",
      "Epoch [10/20], Loss: 0.1047, Validation Acc: 60.09%\n",
      "Epoch [11/20], Loss: 0.0368, Validation Acc: 60.48%\n",
      "Epoch [12/20], Loss: 0.0368, Validation Acc: 60.57%\n",
      "Epoch [13/20], Loss: 0.0286, Validation Acc: 60.54%\n",
      "Epoch [14/20], Loss: 0.0846, Validation Acc: 60.33%\n",
      "Epoch [15/20], Loss: 0.0326, Validation Acc: 60.31%\n",
      "Epoch [16/20], Loss: 0.0325, Validation Acc: 60.52%\n",
      "Epoch [17/20], Loss: 0.0427, Validation Acc: 60.39%\n",
      "Epoch [18/20], Loss: 0.0319, Validation Acc: 60.29%\n",
      "Epoch [19/20], Loss: 0.1085, Validation Acc: 60.66%\n",
      "Epoch [20/20], Loss: 0.0237, Validation Acc: 60.56%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "total_loss = []\n",
    "acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.float(), labels.long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.float(), labels.long()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        acc.append(100 * correct / total)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Acc: {100 * correct / total:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 60.56%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.float(), labels.long()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # define hyperparameters to tune\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [32, 64, 128, 256])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1)\n",
    "\n",
    "    # define the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "    # define the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # train the model\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.float(), labels.long()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.float(), labels.long()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # calculate validation accuracy\n",
    "    val_acc = 100.0 * total_correct / total_samples\n",
    "\n",
    "    return val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 13:03:49,272]\u001b[0m A new study created in memory with name: no-name-c93f77ec-e098-4fee-99c6-7012f9b83754\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:13:03,151]\u001b[0m Trial 0 finished with value: 61.36760668829548 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.08681364710277525}. Best is trial 0 with value: 61.36760668829548.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:18:55,384]\u001b[0m Trial 1 finished with value: 64.81157973546294 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.06945230449415078}. Best is trial 1 with value: 64.81157973546294.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:24:34,055]\u001b[0m Trial 2 finished with value: 61.91664586972798 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.010758727969355912}. Best is trial 1 with value: 64.81157973546294.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:26:28,574]\u001b[0m Trial 3 finished with value: 66.3713501372598 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0520992927851131}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:27:30,709]\u001b[0m Trial 4 finished with value: 66.25904666833043 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.04582559294895239}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:30:16,907]\u001b[0m Trial 5 finished with value: 66.08435238332918 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0444410138888439}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:31:39,564]\u001b[0m Trial 6 finished with value: 62.216121786872975 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.006462120339778028}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:34:31,326]\u001b[0m Trial 7 finished with value: 64.69927626653357 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.07952461834248926}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:37:39,404]\u001b[0m Trial 8 finished with value: 61.36760668829548 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0022698037194206697}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:46:18,041]\u001b[0m Trial 9 finished with value: 59.820314449713 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.09585183453751552}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:48:35,812]\u001b[0m Trial 10 finished with value: 65.98452707761417 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.06249772000028341}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:49:25,345]\u001b[0m Trial 11 finished with value: 65.54779136511105 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.043914114028382426}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:50:16,324]\u001b[0m Trial 12 finished with value: 65.27327177439481 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.032565532006119974}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:51:06,766]\u001b[0m Trial 13 finished with value: 65.93461442475667 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.05817239664418683}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:52:31,683]\u001b[0m Trial 14 finished with value: 64.99875218367856 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.032788078228014286}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:56:01,621]\u001b[0m Trial 15 finished with value: 66.05939605690043 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.053875236330318}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:56:54,646]\u001b[0m Trial 16 finished with value: 65.74744197654105 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.07144066686018945}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 13:58:22,907]\u001b[0m Trial 17 finished with value: 65.64761667082605 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.03487573481110625}. Best is trial 3 with value: 66.3713501372598.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:00:13,941]\u001b[0m Trial 18 finished with value: 66.89543299226354 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.04932015773066042}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:04:27,660]\u001b[0m Trial 19 finished with value: 65.28574993760918 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.06376633168355224}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:06:17,221]\u001b[0m Trial 20 finished with value: 65.53531320189668 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.02149296061991812}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:08:14,051]\u001b[0m Trial 21 finished with value: 65.84726728225606 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.049790826222968224}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:09:50,764]\u001b[0m Trial 22 finished with value: 66.29648115797355 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.046911892336376854}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:11:17,021]\u001b[0m Trial 23 finished with value: 65.66009483404044 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.054580984682306724}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:12:46,941]\u001b[0m Trial 24 finished with value: 65.13601197903668 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.03884747007181501}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:15:02,146]\u001b[0m Trial 25 finished with value: 66.2216121786873 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.05322068575148489}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:16:24,255]\u001b[0m Trial 26 finished with value: 65.88470177189917 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.06059228813178345}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:19:09,431]\u001b[0m Trial 27 finished with value: 65.73496381332667 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.04855136243629166}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:23:57,565]\u001b[0m Trial 28 finished with value: 65.5228350386823 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.02357161726431834}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n",
      "\u001b[32m[I 2023-05-03 14:26:08,952]\u001b[0m Trial 29 finished with value: 65.53531320189668 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.07612479637949726}. Best is trial 18 with value: 66.89543299226354.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# print the best hyperparameters and the best validation accuracy\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best validation accuracy: \", study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
